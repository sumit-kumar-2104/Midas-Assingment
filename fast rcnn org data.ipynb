{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ea6e1-04bb-4acd-b253-25ea0269e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda Software\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda Software\\envs\\tf\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ðŸ”¹ Custom Flickr8K Dataset\n",
    "class Flickr8KDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.image_files = os.listdir(image_folder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_folder, self.image_files[idx].replace('.jpg', '.txt'))\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        h, w, _ = img.shape\n",
    "        boxes, labels = [], []\n",
    "\n",
    "        try:\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f.readlines():\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        print(f\"Skipping malformed line in {label_path}: {line.strip()}\")\n",
    "                        continue  # Skip incorrect lines\n",
    "\n",
    "                    class_id, x_center, y_center, width, height = map(float, parts)\n",
    "                    \n",
    "                    # Convert YOLO format (relative) to absolute pixel coordinates\n",
    "                    x1 = (x_center - width / 2) * w\n",
    "                    y1 = (y_center - height / 2) * h\n",
    "                    x2 = (x_center + width / 2) * w\n",
    "                    y2 = (y_center + height / 2) * h\n",
    "\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    labels.append(int(class_id))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: No label file found for {img_path}, skipping.\")\n",
    "            boxes, labels = [], []\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "        }\n",
    "        \n",
    "        img = F.to_tensor(img)\n",
    "        return img, target\n",
    "\n",
    "# ðŸ”¹ Load Dataset\n",
    "train_dataset = Flickr8KDataset(\"C:\\\\Users\\\\ASUS\\\\Downloads\\\\IIITD Ass\\\\Flickr8k.v3i.yolov8\\\\train\\\\images\", \"C:\\\\Users\\\\ASUS\\\\Downloads\\\\IIITD Ass\\\\Flickr8k.v3i.yolov8\\\\train\\\\labels\")\n",
    "valid_dataset = Flickr8KDataset(\"C:\\\\Users\\\\ASUS\\\\Downloads\\\\IIITD Ass\\\\Flickr8k.v3i.yolov8\\\\valid\\\\images\", \"C:\\\\Users\\\\ASUS\\\\Downloads\\\\IIITD Ass\\\\Flickr8k.v3i.yolov8\\\\valid\\\\labels\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "\n",
    "# ðŸ”¹ Load Faster R-CNN Model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 34  # Your dataset has 34 classes (including background)\n",
    "\n",
    "# Modify the model's classifier head for 34 classes\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "flickr8k_classes = {\n",
    "    0: \"objects\", 1: \"Carton\", 2: \"Hat\", 3: \"Mountains\", 4: \"Pipe\", 5: \"Polythene\", 6: \"Tent\", \n",
    "    7: \"Tub\", 8: \"Backpack\", 9: \"Baseball Glove\", 10: \"Bed\", 11: \"Bicycle\", 12: \"Bird\", 13: \"Boat\", \n",
    "    14: \"Car\", 15: \"Cat\", 16: \"Cell Phone\", 17: \"Couch\", 18: \"Dog\", 19: \"Frisbee\", 20: \"Handbag\", \n",
    "    21: \"Horse\", 22: \"Motorcycle\", 23: \"Mug\", 24: \"Person\", 25: \"Potted Plant\", 26: \"Rope\", \n",
    "    27: \"Skateboard\", 28: \"Sports Ball\", 29: \"Suitcase\", 30: \"Surfboard\", 31: \"Table\", 32: \"Tree\", \n",
    "    33: \"Umbrella\"\n",
    "}\n",
    "\n",
    "# ðŸ”¹ Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# ðŸ”¹ Training Loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for images, targets in train_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "print(\"âœ… Faster R-CNN Training Completed\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f3397-7143-4ad1-9aa4-d809df5e2e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5fed4-e7c5-48ab-977c-8bccfc1c3b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82793ecb-a10c-40cf-b256-ab2bf95e7096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to compute IoU (Intersection over Union)\n",
    "def calculate_iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "# Function to match predictions with ground truth\n",
    "def match_predictions(gt_boxes, gt_labels, pred_boxes, pred_labels, pred_scores, iou_threshold=0.5):\n",
    "    matched_gt = []\n",
    "    matched_pred = []\n",
    "    matched_scores = []\n",
    "    \n",
    "    used_preds = set()\n",
    "\n",
    "    for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "        best_iou = 0\n",
    "        best_pred_idx = -1\n",
    "\n",
    "        for pred_idx, pred_box in enumerate(pred_boxes):\n",
    "            if pred_idx in used_preds:\n",
    "                continue\n",
    "\n",
    "            iou = calculate_iou(gt_box, pred_box)\n",
    "            if iou > best_iou and iou >= iou_threshold:\n",
    "                best_iou = iou\n",
    "                best_pred_idx = pred_idx\n",
    "\n",
    "        if best_pred_idx != -1:\n",
    "            matched_gt.append(gt_labels[gt_idx])\n",
    "            matched_pred.append(pred_labels[best_pred_idx])\n",
    "            matched_scores.append(pred_scores[best_pred_idx])\n",
    "            used_preds.add(best_pred_idx)\n",
    "\n",
    "    return matched_gt, matched_pred, matched_scores\n",
    "\n",
    "# Perform inference on validation set\n",
    "model.eval()\n",
    "all_gt_labels = []\n",
    "all_pred_labels = []\n",
    "all_pred_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in valid_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        for i in range(len(images)):\n",
    "            gt_boxes = targets[i][\"boxes\"].cpu().numpy()\n",
    "            gt_labels = targets[i][\"labels\"].cpu().numpy()\n",
    "\n",
    "            pred_boxes = outputs[i][\"boxes\"].cpu().numpy()\n",
    "            pred_labels = outputs[i][\"labels\"].cpu().numpy()\n",
    "            pred_scores = outputs[i][\"scores\"].cpu().numpy()\n",
    "\n",
    "            matched_gt, matched_pred, matched_scores = match_predictions(gt_boxes, gt_labels, pred_boxes, pred_labels, pred_scores)\n",
    "\n",
    "            all_gt_labels.extend(matched_gt)\n",
    "            all_pred_labels.extend(matched_pred)\n",
    "            all_pred_scores.extend(matched_scores)\n",
    "\n",
    "# Compute mAP (mean Average Precision)\n",
    "def compute_map(gt_labels, pred_labels, pred_scores, num_classes):\n",
    "    ap_per_class = []\n",
    "\n",
    "    for cls in range(1, num_classes):\n",
    "        cls_gt = np.array([1 if label == cls else 0 for label in gt_labels])\n",
    "        cls_pred_scores = np.array([score if label == cls else 0 for label, score in zip(pred_labels, pred_scores)])\n",
    "\n",
    "        if np.sum(cls_gt) > 0:\n",
    "            precision, recall, _ = precision_recall_curve(cls_gt, cls_pred_scores)\n",
    "            ap_score = average_precision_score(cls_gt, cls_pred_scores)\n",
    "            ap_per_class.append(ap_score)\n",
    "\n",
    "    mAP = np.mean(ap_per_class) if ap_per_class else 0\n",
    "    return mAP, ap_per_class\n",
    "\n",
    "num_classes = 34\n",
    "mAP, ap_per_class = compute_map(all_gt_labels, all_pred_labels, all_pred_scores, num_classes)\n",
    "print(f\"ðŸ“Š Mean Average Precision (mAP): {mAP:.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(all_gt_labels, all_pred_labels, labels=list(range(num_classes)))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a55379-e492-44dc-9d76-3d3806409d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83bb5f9-d99e-48ed-b683-2289795df10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, confusion_matrix\n",
    "from collections import defaultdict\n",
    "\n",
    "# ðŸ”¹ Evaluation\n",
    "model.eval()\n",
    "all_preds, all_targets, all_scores = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in valid_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        for output, target in zip(outputs, targets):\n",
    "            threshold = 0.2  # Adjust as needed\n",
    "            scores = output[\"scores\"].cpu().numpy()\n",
    "            labels = output[\"labels\"].cpu().numpy()\n",
    "            \n",
    "            valid_indices = scores > threshold  # Keep only high-confidence detections\n",
    "            scores = scores[valid_indices]\n",
    "            labels = labels[valid_indices]\n",
    "\n",
    "            all_preds.extend(labels)\n",
    "            all_scores.extend(scores)\n",
    "            all_targets.extend(target[\"labels\"].cpu().numpy())\n",
    "\n",
    "# Ensure all_targets and all_preds are of equal length\n",
    "min_len = min(len(all_targets), len(all_preds))\n",
    "all_targets = all_targets[:min_len]  # Trim to match length\n",
    "all_preds = all_preds[:min_len]\n",
    "\n",
    "# Organize scores and targets per class\n",
    "class_scores_dict = defaultdict(list)\n",
    "class_targets_dict = defaultdict(list)\n",
    "\n",
    "for pred_label, pred_score, true_label in zip(all_preds, all_scores, all_targets):\n",
    "    class_scores_dict[true_label].append(pred_score)\n",
    "    class_targets_dict[true_label].append(1)  # Positive class\n",
    "    for other_class in range(num_classes):\n",
    "        if other_class != true_label:\n",
    "            class_targets_dict[other_class].append(0)\n",
    "            class_scores_dict[other_class].append(pred_score)\n",
    "\n",
    "# ðŸ”¹ Compute Precision-Recall per class\n",
    "precision_scores, recall_scores, ap_scores = {}, {}, {}\n",
    "\n",
    "for class_id in class_targets_dict.keys():\n",
    "    y_true = np.array(class_targets_dict[class_id])\n",
    "    y_score = np.array(class_scores_dict[class_id])\n",
    "\n",
    "    if len(y_true) == 0 or len(y_score) == 0:\n",
    "        print(f\"Skipping class {class_id} due to no data\")\n",
    "        continue  # No data for this class\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "\n",
    "    precision_scores[class_id] = precision\n",
    "    recall_scores[class_id] = recall\n",
    "    ap_scores[class_id] = ap\n",
    "\n",
    "# ðŸ”¹ Compute Mean Average Precision (mAP) at IoU 50 and IoU 50-95\n",
    "mAP50 = np.mean(list(ap_scores.values()))\n",
    "mAP50_95 = np.mean(list(ap_scores.values()))  # Placeholder (modify as needed)\n",
    "\n",
    "print(f\"mAP@50: {mAP50:.4f}\")\n",
    "print(f\"mAP@50-95: {mAP50_95:.4f}\")\n",
    "\n",
    "# ðŸ”¹ Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for class_id, precision in precision_scores.items():\n",
    "    recall = recall_scores[class_id]\n",
    "    plt.plot(recall, precision, label=f'Class {class_id}')\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(all_targets, all_preds, labels=list(flickr8k_classes.keys()))\n",
    "print(\"Confusion Matrix:\\n\", cm)  # Print for debugging\n",
    "\n",
    "# Plot Confusion Matrix with Class Names\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=flickr8k_classes.values(), yticklabels=flickr8k_classes.values())\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Save metrics\n",
    "true_positives = sum([sum(class_targets_dict[c]) for c in class_targets_dict])\n",
    "false_positives = len(all_preds) - true_positives\n",
    "false_negatives = len(all_targets) - true_positives\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives + 1e-6)  # Avoid division by zero\n",
    "recall = true_positives / (true_positives + false_negatives + 1e-6)  # Correct recall calculation\n",
    "\n",
    "metrics = {\n",
    "    \"Precision(B)\": precision,\n",
    "    \"Recall(B)\": recall,\n",
    "    \"mAP50(B)\": mAP50,\n",
    "    \"mAP50-95(B)\": mAP50_95\n",
    "}\n",
    "\n",
    "np.save(\"metrics.npy\", metrics)\n",
    "\n",
    "print(\"âœ… Evaluation Completed: Metrics, Precision-Recall Curve & Confusion Matrix Saved\")\n",
    "print(metrics)\n",
    "\n",
    "# Save only model weights\n",
    "torch.save(model.state_dict(), \"faster_rcnn_flickr8k.pth\")\n",
    "\n",
    "# Save entire model (architecture + weights)\n",
    "torch.save(model, \"faster_rcnn_flickr8k_full.pth\")\n",
    "\n",
    "print(\"âœ… Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3248852-a74f-4ac8-a0df-3b1f64a5456d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da79faae-6471-41ea-8d44-77234dd2d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Evaluation\n",
    "model.eval()\n",
    "all_preds, all_targets, all_scores = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in valid_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "\n",
    "        for output, target in zip(outputs, targets):\n",
    "            threshold = 0.2  # Adjust as needed\n",
    "            scores = output[\"scores\"].cpu().numpy()\n",
    "            labels = output[\"labels\"].cpu().numpy()\n",
    "            \n",
    "            valid_indices = scores > threshold  # Keep only high-confidence detections\n",
    "            scores = scores[valid_indices]\n",
    "            labels = labels[valid_indices]\n",
    "\n",
    "\n",
    "            all_preds.extend(labels)\n",
    "            all_scores.extend(scores)\n",
    "            all_targets.extend(target[\"labels\"].cpu().numpy())\n",
    "\n",
    "\n",
    "# Ensure all_targets and all_preds are of equal length\n",
    "min_len = min(len(all_targets), len(all_preds))\n",
    "all_targets = all_targets[:min_len]  # Trim to match length\n",
    "all_preds = all_preds[:min_len]\n",
    "\n",
    "# Organize scores and targets per class\n",
    "class_scores_dict = defaultdict(list)\n",
    "class_targets_dict = defaultdict(list)\n",
    "\n",
    "for pred_label, pred_score, true_label in zip(all_preds, all_scores, all_targets):\n",
    "    class_scores_dict[true_label].append(pred_score)\n",
    "    class_targets_dict[true_label].append(1)  # Positive class\n",
    "    for other_class in range(num_classes):\n",
    "        if other_class != true_label and other_class in labels:  # Ensure only relevant negatives are added\n",
    "            class_targets_dict[other_class].append(0)\n",
    "            class_scores_dict[other_class].append(pred_score)\n",
    "    \n",
    "\n",
    "\n",
    "# ðŸ”¹ Compute Precision-Recall per class\n",
    "precision_scores, recall_scores, ap_scores = {}, {}, {}\n",
    "\n",
    "for class_id in class_targets_dict.keys():\n",
    "    y_true = np.array(class_targets_dict[class_id])\n",
    "    y_score = np.array(class_scores_dict[class_id])\n",
    "\n",
    "    if len(y_true) == 0 or len(y_score) == 0:\n",
    "        print(f\"Skipping class {class_id} due to no data\")\n",
    "        continue  # No data for this class\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "\n",
    "    precision_scores[class_id] = precision\n",
    "    recall_scores[class_id] = recall\n",
    "    ap_scores[class_id] = ap\n",
    "\n",
    "# ðŸ”¹ Compute Mean Average Precision (mAP)\n",
    "mAP50 = np.mean([ap_scores[c] for c in ap_scores if c in ap_scores])\n",
    "mAP50_95 = np.mean([ap_scores[c] for c in ap_scores if c in ap_scores])  # Placeholder (modify as needed)\n",
    "\n",
    "print(f\"mAP@50: {mAP50:.4f}\")\n",
    "print(f\"mAP@50-95: {mAP50_95:.4f}\")\n",
    "\n",
    "# ðŸ”¹ Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "for class_id, precision in precision_scores.items():\n",
    "    recall = recall_scores[class_id]\n",
    "    plt.plot(recall, precision, label=f'Class {class_id}')\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(all_targets, all_preds, labels=list(flickr8k_classes.keys()))\n",
    "print(\"Confusion Matrix:\\n\", cm)  # Print for debugging\n",
    "\n",
    "\n",
    "# Plot Confusion Matrix with Class Names\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=flickr8k_classes.values(), yticklabels=flickr8k_classes.values())\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better visibility\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Save metrics\n",
    "true_positives = sum([sum(class_targets_dict[c]) for c in class_targets_dict])\n",
    "false_positives = sum([len(class_targets_dict[c]) - sum(class_targets_dict[c]) for c in class_targets_dict])\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives + 1e-6)  # Avoid division by zero\n",
    "recall = true_positives / (len(all_targets) + 1e-6)  # Compute recall correctly\n",
    "\n",
    "metrics = {\n",
    "    \"Precision(B)\": precision,\n",
    "    \"Recall(B)\": recall,\n",
    "    \"mAP50(B)\": mAP50,\n",
    "    \"mAP50-95(B)\": mAP50_95\n",
    "}\n",
    "\n",
    "np.save(\"metrics.npy\", metrics)\n",
    "\n",
    "print(\"âœ… Evaluation Completed: Metrics, Precision-Recall Curve & Confusion Matrix Saved\")\n",
    "print(metrics)\n",
    "\n",
    "# Save only model weights\n",
    "torch.save(model.state_dict(), \"faster_rcnn_flickr8k.pth\")\n",
    "\n",
    "# Save entire model (architecture + weights)\n",
    "torch.save(model, \"faster_rcnn_flickr8k_full.pth\")\n",
    "\n",
    "print(\"âœ… Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6700218-592d-418c-aa6f-b53f048fd8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def visualize_predictions(model, dataloader, device, num_images=3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(dataloader):\n",
    "            if i >= num_images:\n",
    "                break\n",
    "            \n",
    "            images = torch.stack([img.to(device) for img in images])\n",
    "            predictions = model(images)\n",
    "\n",
    "            for img, pred, target in zip(images, predictions, targets):\n",
    "                img = img.cpu().permute(1, 2, 0).numpy()  # Convert Tensor to NumPy\n",
    "                img = (img * 255).astype(np.uint8)  # Convert to uint8\n",
    "                \n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)  # OpenCV expects BGR\n",
    "                \n",
    "                # Draw ground truth boxes\n",
    "                for box in target[\"boxes\"].cpu().numpy():\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green for GT\n",
    "                \n",
    "                # Draw predicted boxes\n",
    "                for box, score in zip(pred[:, :4].cpu().numpy(), pred[:, 4].cpu().numpy()):\n",
    "                    if score > 0.3:  # Only draw high-confidence predictions\n",
    "                        x1, y1, x2, y2 = map(int, box)\n",
    "                        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)  # Blue for Predictions\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # Convert BGR back to RGB for display\n",
    "                plt.show()\n",
    "\n",
    "visualize_predictions(model_eval, valid_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
